## tcp
  TCP提供了一种可靠、面向连接、字节流、传输层的服务，采用三次握手建立一个连接。采用4次挥手来关闭一个连接。

## epoll和select
* [深度理解select、poll和epoll](https://blog.csdn.net/davidsguo008/article/details/73556811)

在linux 没有实现epoll事件驱动机制之前，我们一般选择用select或者poll等IO多路复用的方法来实现并发服务程序。在大数据、高并发、集群等一些名词唱得火热之年代，select和poll的用武之地越来越有限，风头已经被epoll占尽。

select()和poll() IO多路复用模型
select的缺点：

单个进程能够监视的文件描述符的数量存在最大限制，通常是1024，当然可以更改数量，但由于select采用轮询的方式扫描文件描述符，文件描述符数量越多，性能越差；(在linux内核头文件中，有这样的定义：#define __FD_SETSIZE    1024)
内核 / 用户空间内存拷贝问题，select需要复制大量的句柄数据结构，产生巨大的开销；
select返回的是含有整个句柄的数组，应用程序需要遍历整个数组才能发现哪些句柄发生了事件；
select的触发方式是水平触发，应用程序如果没有完成对一个已经就绪的文件描述符进行IO操作，那么之后每次select调用还是会将这些文件描述符通知进程。
相比select模型，poll使用链表保存文件描述符，因此没有了监视文件数量的限制，但其他三个缺点依然存在。

拿select模型为例，假设我们的服务器需要支持100万的并发连接，则在__FD_SETSIZE 为1024的情况下，则我们至少需要开辟1k个进程才能实现100万的并发连接。除了进程间上下文切换的时间消耗外，从内核/用户空间大量的无脑内存拷贝、数组轮询等，是系统难以承受的。因此，基于select模型的服务器程序，要达到10万级别的并发访问，是一个很难完成的任务。

因此，该epoll上场了。
epoll IO多路复用模型实现机制
由于epoll的实现机制与select/poll机制完全不同，上面所说的 select的缺点在epoll上不复存在。

设想一下如下场景：有100万个客户端同时与一个服务器进程保持着TCP连接。而每一时刻，通常只有几百上千个TCP连接是活跃的(事实上大部分场景都是这种情况)。如何实现这样的高并发？

在select/poll时代，服务器进程每次都把这100万个连接告诉操作系统(从用户态复制句柄数据结构到内核态)，让操作系统内核去查询这些套接字上是否有事件发生，轮询完后，再将句柄数据复制到用户态，让服务器应用程序轮询处理已发生的网络事件，这一过程资源消耗较大，因此，select/poll一般只能处理几千的并发连接。

epoll的设计和实现与select完全不同。epoll通过在Linux内核中申请一个简易的文件系统(文件系统一般用什么数据结构实现？B+树)。把原先的select/poll调用分成了3个部分：

1）调用epoll_create()建立一个epoll对象(在epoll文件系统中为这个句柄对象分配资源)

2）调用epoll_ctl向epoll对象中添加这100万个连接的套接字

3）调用epoll_wait收集发生的事件的连接

如此一来，要实现上面说是的场景，只需要在进程启动时建立一个epoll对象，然后在需要的时候向这个epoll对象中添加或者删除连接。同时，epoll_wait的效率也非常高，因为调用epoll_wait时，并没有一股脑的向操作系统复制这100万个连接的句柄数据，内核也不需要去遍历全部的连接。

epoll 的特点：epoll 对于句柄事件的选择不是遍历的，是事件响应的，就是句柄上事
件来就马上选择出来，不需要遍历整个句柄链表，因此效率非常高 

## IO 基本概念
Linux 的内核将所有外部设备都可以看做一个文件来操作（Unix 的设计原则，一切皆文件）。那么我们对外部设备的操作都可以看做对文件进行操作。我们对一个文件的读写，都通过调用内核提供的系统调用；内核给我们返回一个 file descriptor（fd，文件描述符）。对一个 socket 的读写也会有相应的描述符，称为socketfd(socket 描述符）。描述符就是一个数字(可以理解为一个索引)，指向内核中一个结构体（文件路径，数据区，等一些属性）。应用程序对文件的读写就通过对描述符的读写完成。

一个基本的 IO，它会涉及到两个系统对象，一个是调用这个 IO 的进程对象，另一个就是系统内核(kernel)。

一般来说，服务器端的 I/O 主要有两种情况：一是来自网络的 I/O；二是对文件(设备)的I/O。

常见的 IO 模型

首先一个 IO 操作其实分成了两个步骤：发起 IO 请求（等待网络数据到达网卡并读取到内核缓冲区，数据准备好）和实际的 IO 操作（从内核缓冲区复制数据到进程空间）。

阻塞和非阻塞

阻塞 IO 和非阻塞 IO 的区别在于第一步：发起 IO 请求是否会被阻塞。如果阻塞直到完成，那么就是传统的阻塞 IO，如果不阻塞，那么就是非阻塞 IO。

同步和异步

同步 IO 和异步 IO 的区别就在于第二个步骤是否阻塞。如果实际的 IO 读写阻塞请求进程，那么就是同步IO。因此常说的阻塞 IO、非阻塞 IO、IO 复用、信号驱动 IO 都是同步 IO。如果不阻塞，而是操作系统帮你做完 IO 操作后再将结果返回给你（通知你），那么就是异步IO。

IO 多路复用

指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。

目前支持 I/O 多路复用的常用系统调用有 select，pselect，poll，epoll 等，I/O 多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但 select，pselect，poll，epoll本质上都是同步 I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步 I/O 则无需自己负责进行读写，异步 I/O 的实现会负责把数据从内核拷贝到用户空间。

5 种 IO 模型

《UNIX 网络编程》对 IO 模型进行了总结，分别是：

阻塞 IO、非阻塞 IO、IO 多路复用、信号驱动的 IO、异步 IO；前 4 种为同步 IO，只有异步 IO 模型是异步 IO。

## 相对于java，Go拥有着超高并发能力，那么Go是如何解决IO等待问题的？
回答：
借此问题，普及一下 IO 的相关知识点（如上述）。

目前很多高性能的基础网络服务器都是采用的 C 语言开发的，比如：Nginx、Redis、memcached 等，它们都是基于”事件驱动 + 事件回调函数”的方式实现，也就是采用 epoll 等作为网络收发数据包的核心驱动。但不少人都认为“事件驱动 + 事件回调函数”的编程方法是“反人类”的；因为大多数人都更习惯线性的处理一件事情：做完第一件事情再做第二件事情，并不习惯在 N 件事情之间频繁的切换干活。为了解决程序员在开发服务器时需要自己的大脑不断的“上下文切换”的问题，Go 语言引入了一种用户态线程 goroutine 来取代编写异步的事件回调函数，从而重新回归到多线程并发模型的线性、同步的编程方式上。

在 Linux 上 Go 语言写的网络服务器也是采用的 epoll 作为最底层的数据收发驱动，Go 语言网络的底层实现中同样存在“上下文切换”的工作，只是这个切换工作由 runtime 的调度器来做了，减少了程序员的负担。

所以，IO 等待是必然，只是谁等的问题。Go 语言在遇到 IO 需要等待时，runtime 会进行调度，语言层面处理这个问题。Go 拥有超高并发能力的关键就在于用户态的 goroutine。

### 

### 

### 

### 

### 

### 

### 

### 

### 